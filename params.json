{"name":"Distributed Expectation Maximization","tagline":"A distributed expectation maximization program to be run on Amazon Elastic MapReduce.","body":"Distributed Expectation Maximization\r\n========================\r\n\r\nJake Leichtling '14\r\n\r\nDerek Salama '14\r\n\r\nComputational Linguistics (COSC 73), Fall 2013\r\n\r\nProfessor Sravana Reddy\r\n\r\nThe MapReduce directory contains the principle output of our project, i.e. a distributed expectation maximization\r\nprogram to estimate hidden markov model parameters. The source code can be found in the src directory, with corresponding\r\ndocumenation in the doc directory. The bin directory contains the program jar, which has been compiled from the source\r\nfiles linked with Hadoop version 2.2.0.\r\n\r\n----------\r\nCompiling\r\n----------\r\nIn order to compile the source files, the build path must contain the hadoop-common-<version>.jar,\r\nhadoop-hdfs-<version>.jar, and hadoop-mapreduce-client-core-<version>.jar libraries, which we have included in the\r\nHadoopJars directory.\r\n\r\nPlease note that we have included a precompiled jar in the MapReduce/bin/ directory. You will only need to compile\r\nif you would like to make modifications.\r\n\r\n--------\r\nRunning\r\n--------\r\nThe current implementation of distributed EM must be run on an Amazon ElasticMapreduce (EMR) cluster with an Amazon S3\r\ndistributed filesystem beneath it. The program jar, input corpora, and paramater seed files must be uploaded\r\nto the S3 filesystem prior to running.\r\n\r\nTo run the jar, find a running EMR cluster or create a new one, and ensure that the cluster is using the Amazon Hadoop\r\ndistribution marked as \"latest\" (this was \"2.4.2 (Hadoop 1.0.3) - latest\" during our development). Add a custom jar\r\nstep and point it to the distributed EM jar (which can be found pre-compiled in the MapReduce/bin/ directory). \r\nThe arguments are described in the EMDriver.java javadoc, but are repeated here for conevenience:\r\n\r\nArguments:\r\n\r\n0: Job name, e.g. \"distributed-hmm-em\"\r\n\r\n1: The bucket URI, e.g. \"s3n://distributed-hmm-em/\" for the file system\r\n\r\n2: Path to input directory containing emission sequences (one per line), e.g. \"s3n://distributed-hmm-em/input\"\r\n\r\n3: Path to output directory that does not yet exist, e.g. \"s3n://distributed-hmm-em/output-13\"\r\n\r\n4: Path to transitions file (each line of format \"<from_state> <to_state>\", with the first from_state\r\nbeing the symbol for the start of the emission sequence, e.g. \"s3n://distributed-hmm-em/transitions.txt\"\r\n     \r\n5: Path to emissions file (each line of format \"<state> <token>\", e.g. \"s3n://distributed-hmm-em/emissions.txt\"\r\n\r\n6: Log convergence, i.e. difference between log alpha of EM iterations before final output is produced,\r\ne.g. \"0.01\"\r\n\r\n7: Max number of EM iterations, or -1 for no maximum, e.g. \"10\"\r\n\r\n8: Number of different random seeds for model parameters, e.g. \"5\"\r\n\r\n9: Flag to enable Viterbi tagging following EM, e.g. a nonzero int (e.g. \"1\" or \"-1\") to enable and \"0\" to disable\r\n\r\n--------------\r\nDocumentation\r\n--------------\r\nThe javadoc documentation for the source code can be found in the MapRedue/doc directory. This documentation can be\r\nopened for easy consumption by opening the index.html file in your favorite web browser.\r\n\r\nAdditionally, our project writeup FinalWriteup.pdf is located in the root directory.\r\n\r\n----------\r\nConclusion\r\n----------\r\nWe hope you find this program useful for speeding up your expectation maximization endeavours! Please email us\r\nwith any questions.\r\n\r\njake.leichtling@gmail.com\r\n\r\ndereksalama@gmail.com\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}